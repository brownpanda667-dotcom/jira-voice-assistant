{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and print the .env file and print the OPENAI_API_KEY variable",
   "id": "934e7cf75ab0a17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:28.820087Z",
     "start_time": "2025-12-09T09:24:28.816692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"OPENAI_API_KEY Length:\", len(openai_api_key))"
   ],
   "id": "4f076ce0d39f0c46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY Length: 164\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hello world with LangChain and OpenAI's GPT-3.5 Turbo",
   "id": "a662908fce303aa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:30.664307Z",
     "start_time": "2025-12-09T09:24:28.836825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response=llm.invoke(\"Hello, world! This is my first call to LangChain using OpenAI's GPT-3.5 Turbo model.\")\n",
    "\n",
    "print(response.content)\n"
   ],
   "id": "af9801424d2c9e7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to LangChain. How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompt Templates",
   "id": "909b2a400baabae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:32.411588Z",
     "start_time": "2025-12-09T09:24:30.670341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}?\",\n",
    ")\n",
    "\n",
    "prompt = template.format(topic=\"chickens\")\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ],
   "id": "bd9bd0ae2c6282d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the chicken join a band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chaining Steps",
   "id": "aa24b9b3d1fe84f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:33.827607Z",
     "start_time": "2025-12-09T09:24:32.425913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = template | llm\n",
    "response = chain.invoke({\"topic\": \"computers\"})\n",
    "print(response.content)"
   ],
   "id": "ff55176cddfb5ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the computer cold?\n",
      "\n",
      "It left its Windows open!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Output Parser",
   "id": "8104fbbad1c3c90e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:34.563448Z",
     "start_time": "2025-12-09T09:24:33.845495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)"
   ],
   "id": "34f4c1c5228b2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? Because the light attracts bugs!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining and using tools",
   "id": "9e706b9bf8888b15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:35.835970Z",
     "start_time": "2025-12-09T09:24:34.579992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# create a list of tools\n",
    "tools = [multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tools.invoke(\"What is 6 multiplied by 7?\")\n",
    "\n",
    "print(response)\n",
    "\n"
   ],
   "id": "7b4bdccef074fade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 55, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CkoFPEkKxqiqoC8rCHeB1F87079jW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b026d-469b-7e92-8d4a-6b2c1210713d-0' tool_calls=[{'name': 'multiply', 'args': {'a': 6, 'b': 7}, 'id': 'call_yZBUI9BGvO7O1IN3xkcdwn0J', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 17, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:35.855704Z",
     "start_time": "2025-12-09T09:24:35.852634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tool_call = response.tool_calls[0]\n",
    "arguments = tool_call[\"args\"]\n",
    "result = multiply.invoke(arguments)\n",
    "print(\"Tool Result:\", result)"
   ],
   "id": "4e867ac95a7b78ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Result: 42\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agent Executor (or in newer versions, a Graph)",
   "id": "601dfebaa04d9538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:35.948346Z",
     "start_time": "2025-12-09T09:24:35.866224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# We already have 'llm' and 'tools' from before\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant who is good at math.\"\n",
    ")"
   ],
   "id": "65a76f47962f2bc9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:24:37.632790Z",
     "start_time": "2025-12-09T09:24:35.954062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We pass a dictionary with a \"messages\" key\n",
    "# The value is a list containing our user message\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is 6 multiplied by 7?\"}]\n",
    "})\n",
    "\n",
    "# Print the last message in the response (which is the AI's final answer)\n",
    "print(response[\"messages\"][-1].content)"
   ],
   "id": "e2b35390ea8b0537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 multiplied by 7 is 42.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Memory",
   "id": "b9719df0dbfe8c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T09:36:49.846426Z",
     "start_time": "2025-12-09T09:36:48.498395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_history = []\n",
    "\n",
    "conversation = {\"role\": \"user\", \"content\": \"Hi, my name is Bob.\"}\n",
    "chat_history.append(conversation)\n",
    "question = chat_history + [{\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
    "print(f\"Question with history: {question}\")\n",
    "response = agent.invoke({\n",
    "    \"messages\": question\n",
    "})\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "chat_history += question\n",
    "chat_history += [{\"role\": \"assistant\", \"content\": response[\"messages\"][-1].content}]\n",
    "print(chat_history)\n",
    "\n"
   ],
   "id": "8a40203563053b88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question with history: [{'role': 'user', 'content': 'Hi, my name is Bob.'}, {'role': 'user', 'content': 'What is my name?'}]\n",
      "Your name is Bob.\n",
      "[{'role': 'user', 'content': 'Hi, my name is Bob.'}, {'role': 'user', 'content': 'Hi, my name is Bob.'}, {'role': 'user', 'content': 'What is my name?'}, {'role': 'assistant', 'content': 'Your name is Bob.'}]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "84d0442d99401de8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
